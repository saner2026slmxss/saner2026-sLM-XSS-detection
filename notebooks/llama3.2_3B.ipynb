{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2461ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    !pip install --upgrade pip\n",
    "    !pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu129\n",
    "    !pip install \"unsloth[cu121]\"\n",
    "    !pip install ipywidgets\n",
    "    !pip install bitsandbytes\n",
    "    !pip install orjson\n",
    "    # !sudo dnf install python3.13-devel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ffbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def chat(user_text, max_new_tokens=64, temperature=0.0, do_sample=False):\n",
    "    messages = [{\"role\": \"user\",\"content\": [{\"type\": \"text\", \"text\": user_text}]}]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt=True, \n",
    "        tokenize=True,\n",
    "        return_dict=True, \n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature, \n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "    return tokenizer.decode(out[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "# print(chat(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d10c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, Any\n",
    "\n",
    "EPOCHS = 12\n",
    "K = 3                    \n",
    "C0 = 1.5                 \n",
    "TRAIN_BS = 256           \n",
    "TEST_BS  = 512           \n",
    "MISMATCH_CAP = 8         \n",
    "NEW_PROMPTS_PER_ROUND = 1\n",
    "BATCH_WRITE = 4096       \n",
    "\n",
    "PROMPT_EVAL_TMPL = \"\"\"# Task:\n",
    "{{prompt}}\n",
    "# Output format:\n",
    "Answer Yes or No as labels\n",
    "# Prediction:\n",
    "Text: {{text}}\n",
    "Label:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_GRAD_TMPL = \"\"\"I am trying to write a prompt to identify attack vectors.\n",
    "My current prompt is:\n",
    "\"{{prompt}}\"\n",
    "However, this prompt incorrectly processes the following example:\n",
    "{{error_string}}\n",
    "Please explain why this prompt might process this example incorrectly.\n",
    "Reason:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_UPDATE_TMPL = \"\"\"I am trying to write a prompt to identify attack vectors.\n",
    "My current prompt is: \"{{prompt}}\"\n",
    "However, the problem with this prompt is: {{reason}}\n",
    "Based on the above issue, Please write {{number_of_new_prompts}} different improved prompts.\n",
    "Each prompt is wrapped with <START> and <END>.\n",
    "These {{number_of_new_prompts}} new prompts are:\n",
    "\"\"\"\n",
    "\n",
    "GEN_ARGS: Dict[str, Any] = {\n",
    "    \"max_new_tokens\": 8, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False\n",
    "}\n",
    "\n",
    "YES_PAT = re.compile(r\"\\s*(Yes|yes|y|true|1)\\b\", re.I)\n",
    "NO_PAT  = re.compile(r\"\\s*(No|no|n|false|0)\\b\", re.I)\n",
    "START_TAG, END_TAG = \"<START>\", \"<END>\"\n",
    "EXTRACT_NEW_PROMPT = re.compile(r\"<START>([\\s\\S]*?)<\\/?END>\", re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32aa5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "def fill_eval(prompt_str, text):\n",
    "    return PROMPT_EVAL_TMPL.replace(\"{{prompt}}\", prompt_str).replace(\"{{text}}\", text)\n",
    "\n",
    "def fill_grad(prompt_str, error_text):\n",
    "    return PROMPT_GRAD_TMPL.replace(\"{{prompt}}\", prompt_str).replace(\"{{error_string}}\", error_text)\n",
    "\n",
    "def fill_update(prompt_str, reason, k):\n",
    "    return (PROMPT_UPDATE_TMPL\n",
    "            .replace(\"{{prompt}}\", prompt_str)\n",
    "            .replace(\"{{reason}}\", reason)\n",
    "            .replace(\"{{number_of_new_prompts}}\", str(k)))\n",
    "\n",
    "def render_eval_input(prompt_str: str, text: str) -> str:\n",
    "    return fill_eval(prompt_str, text)\n",
    "\n",
    "def normalize_yesno(raw: str, inp: str = '') -> str:\n",
    "    if not raw:\n",
    "        return \"no\"\n",
    "    \n",
    "    s = raw.splitlines()[0].strip(\"`'\\\" \").lower()\n",
    "    \n",
    "    if YES_PAT.search(s): return \"yes\"\n",
    "    if NO_PAT.search(s):  return \"no\"\n",
    "    \n",
    "    return \"no\"\n",
    "\n",
    "def llm_yesno(prompt_str: str, text: str) -> str:\n",
    "    inp = render_eval_input(prompt_str, text)\n",
    "    out = chat(inp, **GEN_ARGS)\n",
    "    return normalize_yesno(out, inp)\n",
    "\n",
    "def gen_reason(prompt_str: str, error_text: str) -> str:\n",
    "    grad_inp = fill_grad(prompt_str, error_text)\n",
    "\n",
    "    return chat(grad_inp, max_new_tokens=256, temperature=0.0, do_sample=False)\n",
    "\n",
    "def apply_reason_to_prompt(prompt_str: str, reason: str, k: int = 3) -> str:\n",
    "    upd_inp = fill_update(prompt_str, reason, k)\n",
    "    upd_out = chat(upd_inp, max_new_tokens=1024, temperature=0.0, do_sample=False)\n",
    "\n",
    "    m = EXTRACT_NEW_PROMPT.findall(upd_out)\n",
    "    \n",
    "    for raw_prompt in m:\n",
    "        new_prompt = raw_prompt.strip()\n",
    "        if len(new_prompt) >= 8:\n",
    "            return raw_prompt\n",
    "\n",
    "    return (prompt_str + \" Only consider code that executes in a web browser context. Answer Yes or No.\")\n",
    "\n",
    "\n",
    "def sample_batch(dataset, n: int):\n",
    "    if len(dataset) <= n: return dataset\n",
    "    return random.sample(dataset, n)\n",
    "\n",
    "def eval_f1_batch(prompt_str: str, batch):\n",
    "    ys, yh = [], []\n",
    "    for ex in batch:\n",
    "        yh.append(1 if llm_yesno(prompt_str, ex[\"text\"]) == \"yes\" else 0)\n",
    "        ys.append(1 if ex[\"label\"].lower() == \"yes\" else 0)\n",
    "    return f1_score(ys, yh)\n",
    "\n",
    "def evaluate_full(prompt_str: str, dataset):\n",
    "    y_true, y_pred = [], []\n",
    "    for ex in dataset:\n",
    "        y_pred.append(1 if llm_yesno(prompt_str, ex[\"text\"]) == \"yes\" else 0)\n",
    "        y_true.append(1 if ex[\"label\"].lower() == \"yes\" else 0)\n",
    "    cm  = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "    tp, fn, fp, tn = int(cm[0,0]), int(cm[0,1]), int(cm[1,0]), int(cm[1,1])\n",
    "    acc = (tp+tn)/max(1,(tp+tn+fp+fn))\n",
    "    f1  = f1_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    return {\"acc\":acc, \"f1\":f1, \"mcc\":mcc, \"tp\":tp, \"fp\":fp, \"tn\":tn, \"fn\":fn}\n",
    "\n",
    "class UCBR:\n",
    "    def __init__(self, prompts, c0=C0):\n",
    "        self.P = prompts[:]         \n",
    "        self.s = [1.0] * len(self.P)\n",
    "        self.f = [1.0] * len(self.P)\n",
    "        self.N = 1                  \n",
    "        self.c0 = c0\n",
    "        \n",
    "    def select_k(self, k: int):\n",
    "        c = self.c0 / math.sqrt(self.N)\n",
    "        ucb = []\n",
    "        for si, fi in zip(self.s, self.f):\n",
    "            t = si + fi\n",
    "            score = (si / t) + c * math.sqrt(max(1e-9, math.log(self.N) / t))\n",
    "            ucb.append(score)\n",
    "            \n",
    "        weights = [max(1e-9, x) for x in ucb]\n",
    "        idxs = random.choices(range(len(self.P)), weights=weights, k=min(k, len(self.P)))\n",
    "        \n",
    "        seen = set()\n",
    "        out = []\n",
    "        for i in idxs:\n",
    "            if i not in seen:\n",
    "                seen.add(i)\n",
    "                out.append(i)\n",
    "        return out\n",
    "        \n",
    "    def update(self, i: int, score: float, batch_len: int):\n",
    "        self.s[i] += score * batch_len\n",
    "        self.f[i] += (1.0 - score) * batch_len\n",
    "        self.N += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717febbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "LOGDIR = pathlib.Path(\"./artifacts_gemma3\")\n",
    "\n",
    "def preview_tail(path, n=10):\n",
    "    try:\n",
    "        lines = pathlib.Path(path).read_text(encoding=\"utf-8\").splitlines()\n",
    "        for line in lines[-n:]:\n",
    "            print(line)\n",
    "    except Exception as e:\n",
    "        print(f\"(preview error) {e}\")\n",
    "\n",
    "\n",
    "def train_ucbr_prompts_with_logging(base_prompt, train_set, test_set,\n",
    "                                    train_bs=TRAIN_BS, test_bs=TEST_BS,\n",
    "                                    mismatch_cap=MISMATCH_CAP, new_prompts_per_round=NEW_PROMPTS_PER_ROUND,\n",
    "                                    k_select=K, epochs=EPOCHS,\n",
    "                                    log_mismatch_from=\"test\", \n",
    "                                    mismatch_log_sample=64):\n",
    "\n",
    "    EPOCH_PROMPTS_PATH = LOGDIR / \"epoch_prompts.jsonl\"\n",
    "    MISMATCH_SAMPLES_PATH = LOGDIR / \"mismatches_sampled.jsonl\"\n",
    "    \n",
    "    P = [base_prompt]\n",
    "    ucbr = UCBR(P, c0=C0) \n",
    "    best = {\"prompt\": P[0], \"f1\": -1.0}\n",
    "\n",
    "    if LOGDIR.exists():\n",
    "        EPOCH_PROMPTS_PATH.write_text(\"\", encoding=\"utf-8\")\n",
    "        MISMATCH_SAMPLES_PATH.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "    for t in range(1, epochs + 1):\n",
    "        print(f\"[epoch {t:02}] start  arms={len(P)}\", flush=True)\n",
    "        Pexp = P[:]\n",
    "\n",
    "        for i, p in enumerate(P):\n",
    "            train_batch = sample_batch(train_set, train_bs) \n",
    "            \n",
    "            mismatches = []\n",
    "            for ex in train_batch:\n",
    "                pred = llm_yesno(p, ex[\"text\"])\n",
    "                gold = \"yes\" if ex[\"label\"].lower() == \"yes\" else \"no\"\n",
    "                if pred != gold:\n",
    "                    mismatches.append({\"id\": ex.get(\"id\"), \"text\": ex[\"text\"], \"gold\": gold, \"pred\": pred})\n",
    "                if len(mismatches) >= mismatch_cap:\n",
    "                    break\n",
    "\n",
    "            if mismatches and (len(Pexp) - len(P)) < new_prompts_per_round:\n",
    "                reason = gen_reason(p, mismatches[0][\"text\"])\n",
    "                p_new  = apply_reason_to_prompt(p, reason, k = new_prompts_per_round)\n",
    "                Pexp.append(p_new)\n",
    "\n",
    "        idxs = ucbr.select_k(k_select)\n",
    "        test_batch = sample_batch(test_set, test_bs)\n",
    "        print(f\"[epoch {t:02}] select={idxs} n_test={len(test_batch)}\", flush=True)\n",
    "\n",
    "        f1_by_arm = {}\n",
    "        for i in idxs:\n",
    "            p_cur = Pexp[i]\n",
    "            f1 = eval_f1_batch(p_cur, test_batch)\n",
    "            f1_by_arm[i] = f1\n",
    "            ucbr.update(i, f1, len(test_batch))\n",
    "            if f1 > best[\"f1\"]:\n",
    "                best = {\"prompt\": p_cur, \"f1\": f1}\n",
    "\n",
    "        with open(EPOCH_PROMPTS_PATH, \"a\", encoding=\"utf-8\") as fw:\n",
    "            for i, ptxt in enumerate(Pexp):\n",
    "                rec = {\n",
    "                    \"epoch\": t,\n",
    "                    \"arm_index\": i,\n",
    "                    \"prompt\": ptxt,\n",
    "                    \"selected\": int(i in idxs),\n",
    "                    \"f1_if_selected\": f1_by_arm.get(i, None),\n",
    "                }\n",
    "                fw.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        def _collect_mismatches_for_prompt(prompt_text, pool, cap):\n",
    "            rows = []\n",
    "            for ex in pool:\n",
    "                txt = ex.get(\"text\", \"\")\n",
    "                if not txt or not txt.strip(): \n",
    "                    continue\n",
    "                \n",
    "                pred = llm_yesno(prompt_text, ex[\"text\"])\n",
    "                gold = \"yes\" if ex[\"label\"].lower() == \"yes\" else \"no\"\n",
    "                if pred != gold:\n",
    "                    short = ex[\"text\"][:220].replace(\"\\n\",\" \")\n",
    "                    rows.append({\n",
    "                        \"epoch\": t,\n",
    "                        \"prompt\": prompt_text,\n",
    "                        \"id\": ex.get(\"id\"),\n",
    "                        \"label\": gold,\n",
    "                        \"pred\": pred,\n",
    "                        \"text_preview\": short\n",
    "                    })\n",
    "                if len(rows) >= cap:\n",
    "                    break\n",
    "            return rows\n",
    "\n",
    "        for i in idxs:\n",
    "            p_cur = Pexp[i]\n",
    "            pool = test_batch if log_mismatch_from == \"test\" else sample_batch(train_set, mismatch_log_sample)\n",
    "            rows = _collect_mismatches_for_prompt(p_cur, pool, mismatch_log_sample)\n",
    "            if rows:\n",
    "                with open(MISMATCH_SAMPLES_PATH, \"a\", encoding=\"utf-8\") as fw:\n",
    "                    for r in rows:\n",
    "                        fw.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        P = Pexp\n",
    "        ucbr.P = P\n",
    "        if len(ucbr.s) < len(P):\n",
    "            add = len(P) - len(ucbr.s)\n",
    "            ucbr.s += [1.0] * add\n",
    "            ucbr.f += [1.0] * add\n",
    "\n",
    "    return best, str(EPOCH_PROMPTS_PATH), str(MISMATCH_SAMPLES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ea358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random, pathlib, numpy as np\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def set_seed(seed=7):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "set_seed(777)\n",
    "\n",
    "def load_jsonl(p):\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                yield json.loads(line)\n",
    "\n",
    "DEV_JSONL  = \"/home/user/llm/dataset_xsshield/dev.jsonl\" \n",
    "EVAL_JSONL = \"/home/user/llm/dataset_xsshield/eval.jsonl\"\n",
    "\n",
    "try:\n",
    "    train = list(load_jsonl(DEV_JSONL))\n",
    "    test  = list(load_jsonl(EVAL_JSONL))\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] Data files not found. Please check paths: {DEV_JSONL} and {EVAL_JSONL}\")\n",
    "    train = [{\"text\": \"test data for error\", \"label\": \"no\"}] * 10\n",
    "    test = train\n",
    "\n",
    "print(f\"[data] train={len(train)}  test={len(test)}\")\n",
    "print(\"[data] test label dist:\", Counter([x[\"label\"].lower() for x in test]))\n",
    "\n",
    "def clean_dataset_in_place(dataset, name=\"Dataset\"):\n",
    "    original_len = len(dataset)\n",
    "    \n",
    "    cleaned_dataset = [\n",
    "        item for item in dataset \n",
    "        if item.get(\"text\", \"\").strip()\n",
    "    ]\n",
    "    \n",
    "    removed_count = original_len - len(cleaned_dataset)\n",
    "    if removed_count > 0:\n",
    "        pass\n",
    "    return cleaned_dataset\n",
    "\n",
    "train = clean_dataset_in_place(train, name=\"Train set\")\n",
    "test = clean_dataset_in_place(test, name=\"Test set\")\n",
    "\n",
    "SEED_PROMPT = \"Is This JavaScript code slice XSS attack payload?\"\n",
    "\n",
    "LOGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best, ep_path, mm_path = train_ucbr_prompts_with_logging(\n",
    "    SEED_PROMPT, train, test,\n",
    "    train_bs=TRAIN_BS, test_bs=TEST_BS,\n",
    "    mismatch_cap=MISMATCH_CAP, new_prompts_per_round=NEW_PROMPTS_PER_ROUND,\n",
    "    k_select=K, epochs=EPOCHS,\n",
    "    log_mismatch_from=\"test\",\n",
    "    mismatch_log_sample=64,\n",
    ")\n",
    "print(f\"[best F1 est] {best['f1']}\")\n",
    "print(f\"[saved] {ep_path}\")\n",
    "print(f\"[saved] {mm_path}\")\n",
    "\n",
    "CACHE = {}\n",
    "def chat_cached(inp, **kw):\n",
    "    h = hash(inp)\n",
    "    if h in CACHE: return CACHE[h]\n",
    "    out = chat(inp, **kw)\n",
    "    CACHE[h] = out\n",
    "    return out\n",
    "\n",
    "def worker_llm_yesno(prompt_str, text):\n",
    "    if not text or not text.strip():\n",
    "        return \"no\"\n",
    "    \n",
    "    inp = render_eval_input(prompt_str, text)\n",
    "    out = chat_cached(inp, **GEN_ARGS) \n",
    "    return normalize_yesno(out)\n",
    "\n",
    "BEST_PATH = LOGDIR / \"best_prompt.txt\"\n",
    "PARTIAL_FILE = LOGDIR / \"predictions_eval_partial.jsonl\"\n",
    "FINAL_FILE   = LOGDIR / \"predictions_eval.jsonl\"\n",
    "\n",
    "LOGDIR.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "best_prompt_str = best[\"prompt\"]\n",
    "BEST_PATH.write_text(best_prompt_str, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "print(f\"[best F1 est] {best['f1']:.4f} (Note: F1 is from final eval or last batch if resuming)\")\n",
    "print(f\"[saved] {ep_path}\")\n",
    "print(f\"[saved] {mm_path}\")\n",
    "\n",
    "start_idx = 0\n",
    "existing = {}\n",
    "if PARTIAL_FILE.exists():\n",
    "    print(\"[resume] loading partial preds...\", flush=True)\n",
    "    with open(PARTIAL_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i,line in enumerate(f):\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                existing[int(obj[\"_idx\"])] = obj\n",
    "            except:\n",
    "                continue\n",
    "    start_idx = max(existing.keys()) + 1 if existing else 0\n",
    "    print(f\"[resume] loaded {len(existing)} rows; will start at idx={start_idx}\", flush=True)\n",
    "\n",
    "n = len(test)\n",
    "best_prompt_str = best[\"prompt\"]\n",
    "print(f\"[eval] total={n}  start_idx={start_idx}  mode=SEQUENTIAL\", flush=True)\n",
    "\n",
    "processed_count = 0\n",
    "\n",
    "with open(PARTIAL_FILE, \"a\", encoding=\"utf-8\") as outf:\n",
    "    for idx in range(start_idx, n):\n",
    "        ex_obj = test[idx]\n",
    "\n",
    "        if not ex_obj.get(\"text\", \"\").strip():\n",
    "            pred = \"no\"\n",
    "        else:\n",
    "            try:\n",
    "                pred = worker_llm_yesno(best_prompt_str, ex_obj[\"text\"])\n",
    "            except Exception as e:\n",
    "                pred = \"no\"\n",
    "\n",
    "        rec = {\"_idx\": idx, \"label\": ex_obj[\"label\"].lower(), \"pred\": pred}\n",
    "        outf.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        processed_count += 1\n",
    "        if processed_count % BATCH_WRITE == 0:\n",
    "            outf.flush()\n",
    "            print(f\"[INFO] Processed {processed_count} samples (Total {idx+1}/{n})...\", flush=True)\n",
    "\n",
    "print(\"[eval] finished writing partial file. Now aggregate to final.\", flush=True)\n",
    "\n",
    "rows=[]\n",
    "with open(PARTIAL_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            rows.append(json.loads(line))\n",
    "        except:\n",
    "            pass\n",
    "rows.sort(key=lambda x: x[\"_idx\"])\n",
    "\n",
    "y_true = [1 if r[\"label\"]==\"yes\" else 0 for r in rows]\n",
    "y_pred = [1 if r[\"pred\"]==\"yes\" else 0 for r in rows]\n",
    "\n",
    "if len(y_true) > 0:\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "    tp, fn, fp, tn = int(cm[0,0]), int(cm[0,1]), int(cm[1,0]), int(cm[1,1])\n",
    "    acc = (tp+tn)/len(y_true)\n",
    "    f1  = f1_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    with open(FINAL_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\n[FINAL] acc={acc:.4f}  f1={f1:.4f}  mcc={mcc:.4f}\")\n",
    "    print(f\"[CM] tp={tp} fp={fp} tn={tn} fn={fn}\")\n",
    "    print(f\"[saved] {BEST_PATH}\")\n",
    "    print(f\"[saved] {FINAL_FILE}\")\n",
    "else:\n",
    "    print(\"\\n[FINAL] Not enough data processed for final metrics.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
